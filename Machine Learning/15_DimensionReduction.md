# 차원축소
- 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성

#### 장점
- 직관적인 데이터 해석 가능 
    - 수십 개 이상의 피처가 있는 데이터는 시각적으로 표현시 데이터의 특성 파악 불가
    - 3차원 이하의 차원 축소를 통해서 시각적으로 데이터를 압축해서 표현 가능
- 학습에 필요한 처리 능력도 줄일 수 잇음

## 방법
### 1. 피처 선택
- 특정 피처에 종속성이 강한 불필요한 피처는 제거하고 데이터의 특징을 잘 나타내는 주요 피처만 선택
#### 피처 선택 방식
- Filtering
    - 타겟을 잘 설명할 수 있는 변수만 선택
- Wrapper
    - 다양한 변수 조합 중 모델 성능이 좋은 변수 조합을 찾음
    - backward selection
    - forward selection
    - stepwise selectioni
- Embedded
    - 학습알고리즘 자체에 feature selection을 포함하는 경우

### 2. 피처 추출
- 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것
- 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 됨
- 기존의 피처를 단순 압축하는 것이 아니라 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것
- ex. 학생 평가 요소
    - 모의고사 성적, 내신성적, 봉사활동, 대외활동 등
    - 함축적 요약 특성으로 추출: 학업성취도, 문제해결력, 커뮤니케이션 능력
    - 함축적인 특성 추출: 기존 피처가 전혀 인지하기 어려웠던 잠재적인 요소를 추출하는 것
#### 피처 추출 방식
- PCA(Principal Component Analysis)
- SVD(Singular Value Decomposition)
- LDA(Linear Disciminant Analysis)
- NMF(Non-negative Matrix Factorization)

## 차원 축소의 의미와 활용
- 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출
### 활용 분야
- 매우 많은 차원ㅇ르 가지고 있는 이미지나 텍스트 처리 분야에서 잘 활용됨
#### 이미지 영역
- 변환된 이미지는 원본이미지보다 훨씬 적은 차원이기 때문에 이미지 분류 시에 과적합 영향력이 작아져서 오히려 원본 데이터로 예측하는 것보다 예측 성능을 더 끌얼 올릴 수 있음
#### 테긋트 영역
- 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱 의미나 토픽을 잠재 요소로 간주하고 찾아낼 수 있음
- SVD와 NMF는 시맨틱 토픽 모델링을 위한 기반 알고리즘으로 사용됨


## 대표적인 차원 축소 알고리즘
### 1. PCA(Principal Component Analysis)
- 상관이 있는 변수들로 구성된 자료들을 서로 독립인 변수(주성분)로 변환
- 관측된 변수들의 선형결합(변수 간의 중요성이 있음)
- 공분산 행렬에서 고유벡터/고유값을 구하고 분산이 큰 방향을 가진 고유벡터에 입력데이터를 선형변환 (고유값 분해)
- 입력 데이터의 변동성이 가장 큰 축을 구하고, 다시 이 축에 직각인 축을 반복적으로 축소하려는 차원 개수만큼 구한 뒤 입력 데이터를 이 축들에 투영해 차원을 축소하는 방식
- 이를 위해 입력 데이터의 공분산 행렬을 기반으로 고유 벡터를 생성하고 이렇게 구한 고유 벡터에 입력 데이터를 선형 변환하는 방식

### 2. 요인분석(Factor Analysis)
- 수많은 변수 중 관측되지 않은 잠재적인 공통인자를 찾아내 해석하는 방법
- 공통요인들의 선형결합을 통해 소수의 새로운 변수를 생성
- 변수간의 순서가 없음
- 변수들은 기본적으로 대등한 관계

#### PCA와 FA 차이점
|주성분분석(PCA)|요인분석(FA)|
|---------|--------|
|목표변수를 고려하여 목표변수를 잘 예측/분류할 수 있는 선형결합으로 이루어진 몇 개의 주성분을 찾아냄|목표변수를 고려하지 않고 주어진 데이터들간의 상관성을 토대로 비슷한 변수들을 묶어서 새로운 잠재변수를 만들어냄|
|통계적 모형을 가지고 있지 않으며 단순히 자료의 차원 축소 방법으로 관찰변수의 차원보다 적은 차원으로 자료를 기술하려는 기법|공통요인과 고유요인에 의해 예측되는 통계적 모형|
|개념적 상관성보다는 수치적 상관성이 있는지(선형적 데이터 변화량)를 고려하여 뚜렷하게 관찰되는 대표적인 변화량을 주성분으로 통합|전체 변수들 중에서 개념적/논리적으로 주제가 비슷한 몇몇 변수들을 잠재적 요인으로 통합|
|보통 2~3개 성분을 추출|제한없이 여러 개 요인을 도출하며 하나의 요인에 결합되는 변수들의 갯수도 서로 다를 수 있음|

### 3. LDA(Linear Discriminant Analysis)
- 선형 판별 분석법
- PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법
- 중요한 차이는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원 축소
#### PCA와 LDA 차이점
| PCA | LDA |
|--------|-------|
|입력 데이터의 변동성의 가장 큰 축을 찾음|입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾음|
|공분산행렬을 이용|클래스 간 분산과 클래스 내부 분산행렬을 기반으로 하여 고유벡터를 구하고 입력 데이터를 투영|
#### LDA 구하는 단계
1. 클래스 내부와 클래스 간 분산 행렬을 구한다. 이 두 개의 행렬은 입력데이터의 결정 값 클래스 별로 개별 피처의 평균벡터를 기반으로 구한다.
2. 클래스 내부 분산 행렬과 클래스간 분산행렬의 곱을 고유벡터로 분해한다.
3. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼)추출한다.
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다.

### 4. SVD와 NMF
- 매우 많은 피처 데이터를 가진 고차원 행렬을 두 개의 저차원 행렬로 분리하는 행렬 분해 기법
#### SVD(Singula Value Decomposition)
- PCA와 유사한 행렬 분해 기법
- PCA의 경우 정방행렬만을 고유벡터로 분해하지만 SVD는 정방행렬이 아닌 m행 n열의 다양한 행렬에 대한 고유값 분해 일반화 버전
- 자료행렬의 특성값분해로 주성분 도출 가능
- 데이터 용량을 줄이거나 압축을 위해 사용

#### NMF(Non-negative Matrix Factorizatioin)
- 원본 행렬 내의 모든 원소 값이 모두 양수(0 이상)라는 게 보장되면 좀더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법

#### PCA와 SVD
- 많은 차원을 가지는 이미지나 텍스트에서 활발하게 사용됨