## 머신러닝
- 데이터와 데이터로부터 기대되는 답을 주어 실행 규칙을 찾는 방법

- 의미있는 데이터로의 반환
    - **입력 데이터**를 기반으로 **기대출력**에 가깝게 만드는 유용한 표현을 **학습**하는 것
- 학습: 더 나은 표현으로 바꾸어주는 변환을 찾는 자동화된 과정

# 딥러닝
- 머신러닝의 특정한 한 분야
- 연속된 층(layer)에서 점진적으로 의미있는 표현을 배우는 것
- **층 기반 표현 학습** 또는 **계층적 표현 학습**이라고 부르기도 함
- 기본 층을 겹겹이 쌓아 올려 구성된 신경망이라는 모델을 사용

## 층(layer)
- 신경망의 핵심 구성 요소
- 데이터 처리(정제) 필터 기능을 함
    - 데이터가 입력되면 더 유용한 형태로 출력
    - 입력된 데이터로부터 주어진 문제에 더 의미있는 표현 추출
- 딥러닝 모델은 데이터 정체 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같음

## 작동 원리
- 머신러닝은 많은 입력과 타깃의 샘플을 관찰하면서 입력을 타깃에 매핑하는 것
- 심층 신경망은 이런 입력-타깃 매핑을 간단한 데이터 변환기(층)를 많이 연결하여 수행
- 데이터 변환은 샘플에 노출됨으로써 학습이 이루어짐

<br>

1. 신경망은 가중치를 파라미터로 가진다.
- 심층 신경망은 수천만 개의 파라미터를 가지기도 함
- 모든 파라미터의 정확한 값을 찾는 것은 어려운 일로 보임
- 파라미터 하나의 값을 바꾸면 다른 모든 파라미터에 영향을 미침

2. 손실함수가 신경망의 출력 품질을 측정한다.
- 신경망의 출력을 제어하려면 출력이 기대하는 것보다 얼마나 벗어났는지를 측정해야 함
    - 신경망의 손실 함수(loss function) 또는 목적 함수(objective function) 담당
- 신경망이 한 샘플에 대해 얼마나 잘 예측했는지 측정하기 위해 손실 함수가 신경망의 예측과 진짜 타깃의 차이를 점수로 계산함

3. 손실점수를 피드백 신호로 사용하여 가중치를 조정한다.
- 손실 점수를 피드백 신호로 사용하여 현재 샘플의 손실 점수가 감소되는 방향으로 가중치 값을 수정
- 이런 수정 과정은 딥러닝의 핵심 알고리즘인 **역전파(Backpropagation) 알고리즘**을 구현한 **옵티마이저(optimizer)** 가 담당함

## 특징
1. 데이터의 좋은 표현을 만드는 특성 공학을 자동화함
- 특성을 직접 찾는 대신 한 번에 모든 특성을 학습할 수 있음
- 머신러닝 작업 흐름을 매우 단순화함
- 고도의 다단계 작업 과정을 하나의 간단한 엔드-투-엔드(end-to-end) 딥러닝 모델로 대체할 수 있음

2. 층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다는 것과 이런 점진적인 중간 표현이 공동으로 학습됨

3. 각 층은 상위 층과 하위 층의 표현이 변함에 따라서 함께 바뀜

## 학습 과정
1단계. 데이터 준비<br>

2단계. 모델 컴파일
- 활성화함수, 손실함수, 옵티마이저 선택<br>

3단계. 모델 훈련<br>

4단계. 모델 예측

<br>

# 퍼셉트론(Perceptron)
- 신경망을 이루는 가장 중요한 기본 단위
- 입력값과 활성화 함수를 사용해 출력값을 다음으로 넘기는 가장 작은 신경망 단위
- 가중치(weight)는 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수
    - 가중치가 각각의 입력신호에 부여되는 입력신호와 계산을 하고 신호의 총합이 정해진 임계값을 넘으면 다음 뉴런으로 신호를 전달하고 아니면 아무것도 수행하지 않음
    - 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 **활성함수**에 의해 판단됨

- 가중합
$$w_1x_1+w_2x_2+b$$
    - 기울기: 가중치를 의미하는 $w$(weight)
    - 절편: $b$(bias)

#### 활성화 함수(activation function)
- 가중합의 결과를 놓고 1 또는 0을 출력해서 다음으로 보냄
- 여기서 0과 1을 판단하는 함수

## 퍼셉트론의 변화 => 층 추가
- **은닉층(hidden layer)** 을 만들면 됨
- 입력층->은닉층->출력층으로 결과값 전달

<br>

# 오차 역전파(Backpropagation)
## 가중치 조정
#### (1) 단일 퍼셉트론 가중치 조정
- 결과값을 얻으면 실제값과의 오차를 구해 앞 단계에서 정한 가중치를 조정

#### (2) 다층 퍼셉트론의 가중치 조정
- 가중치 업데이트: 출력층과 은닉층의 가중치 수정
- 결과값의 오차를 구해 이를 토대로 하나 앞선 가중치를 차례로 거슬러 올라가며 조정함

### 오차 역전파 동작 단계
1. 임의의 초기 가중치($W$)를 준 뒤 결과($y_{out}$)를 계산
2. 계산 결과(예측값)와 원하는 값(실제값, 타깃값) 사이의 오차 계산
3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트
4. 위 과정을 오차가 줄어들지 않을 때까지 반복

**오차가 작아지는 방향**<br>
⇔ 기울기가 0이 되는 방향

<br>

# 기울기 소실 문제와 활성화 함수
## 기울기 소실 문제
- 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제가 발생

## 활성화 함수
### 시그모이드(sigmoid) 함수
- 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워짐

<br>

⇒ 기울기 소실 문제를 해결하기 위해 시그모이들 대체하는 활성화 함수들
### 1. 하이퍼볼릭 탄젠트(tanh) 함수
- 시그모이드 함수의 범위를 -1에서 1로 확장
- 미분시 1보다 작은 값이 존재하여 기울기 소실 문제는 사라지지 않음

### 2. 렐루(ReLU) 함수
- 0미만의 값은 모두 0으로 처리하고, 0 이상의 값은 원본 값 그대로 출력
- 현재 가장 많이 사용되는 활성화 함수
- 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음

### 3. 소프트플러스(softplus) 함수
- 렐루의 0이 되는 순간을 완화

<br>

# 경사하강법과 옵티마이저
- 속도와 정확도 문제를 해결
### 확률적 경사하강법(SGD)
- 속도 개선
```python
keras.optimizer.SGD(lr=0.1)
```

### 모멘텀(momentum)
- 정확도 개선
- 관성의 방향을 고려해 진동과 폭을 줄이는 효과
- 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고하여 같은 방향으로 일정한 비율만 수정
- 이전 이동 값을 고려하여 일정 비율만큼만 다음 값을 결정하므로 관성의 효과를 낼 수 있음
```python
keras.optimizer.SGD(lr=0.1, momentum=0.9)
```

### 네스테로프 모멘텀(NAG, Nesterov Accelrated Gradient)
- 정확도 개선
- 모멘텀이 이동시킬 방향으로 미리 이동해서 그레디언트 계산
- 불필요한 이동을 줄이는 효과
```python
keras.optimizer.SGD(lr=0.1, momentum=0.9, nestrov=True)
```

### 아다그라드(Adagrad, Adaptive Gradient)
- 보폭 크기 개선
- 변수의 업데이트가 잦을 경우 학습률을 적게하여 이동 보폭을 조절하는 방법
```python
keras.optimizer.Adagrad(lr=0.01, epsilon=1e-6)
```
### RMSprop
- 보폭 크기 개선
- 아다그라드의 보폭 민감도를 보완하는 방법
```python
keras.optimizer.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)
```

### 아담(Adam, Adaptive Moment Estimation)
- 정확도와 보폭 크기 개선
- 모멘텀과 RMSprop 방법을 결합한 방법
- 자주 사용
```python
keras.optimizer.Adam(lr=0.001, beta_1=0.9, beta_2=0.9999, epsilon=1e-08, decay=0.0)
```