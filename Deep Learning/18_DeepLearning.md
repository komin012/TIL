## 머신러닝
- 데이터와 데이터로부터 기대되는 답을 주어 실행 규칙을 찾는 방법

- 의미있는 데이터로의 반환
    - **입력 데이터**를 기반으로 **기대출력**에 가깝게 만드는 유용한 표현을 **학습**하는 것
- 학습: 더 나은 표현으로 바꾸어주는 변환을 찾는 자동화된 과정

# 딥러닝
- 머신러닝의 특정한 한 분야
- 연속된 층(layer)에서 점진적으로 의미있는 표현을 배우는 것
- **층 기반 표현 학습** 또는 **계층적 표현 학습**이라고 부르기도 함
- 기본 층을 겹겹이 쌓아 올려 구성된 신경망이라는 모델을 사용

## 층(layer)
- 신경망의 핵심 구성 요소
- 데이터 처리(정제) 필터 기능을 함
    - 데이터가 입력되면 더 유용한 형태로 출력
    - 입력된 데이터로부터 주어진 문제에 더 의미있는 표현 추출
- 딥러닝 모델은 데이터 정체 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같음

## 작동 원리
- 머신러닝은 많은 입력과 타깃의 샘플을 관찰하면서 입력을 타깃에 매핑하는 것
- 심층 신경망은 이런 입력-타깃 매핑을 간단한 데이터 변환기(층)를 많이 연결하여 수행
- 데이터 변환은 샘플에 노출됨으로써 학습이 이루어짐

<br>

1. 신경망은 가중치를 파라미터로 가진다.
- 심층 신경망은 수천만 개의 파라미터를 가지기도 함
- 모든 파라미터의 정확한 값을 찾는 것은 어려운 일로 보임
- 파라미터 하나의 값을 바꾸면 다른 모든 파라미터에 영향을 미침

2. 손실함수가 신경망의 출력 품질을 측정한다.
- 신경망의 출력을 제어하려면 출력이 기대하는 것보다 얼마나 벗어났는지를 측정해야 함
    - 신경망의 손실 함수(loss function) 또는 목적 함수(objective function) 담당
- 신경망이 한 샘플에 대해 얼마나 잘 예측했는지 측정하기 위해 손실 함수가 신경망의 예측과 진짜 타깃의 차이를 점수로 계산함

3. 손실점수를 피드백 신호로 사용하여 가중치를 조정한다.
- 손실 점수를 피드백 신호로 사용하여 현재 샘플의 손실 점수가 감소되는 방향으로 가중치 값을 수정
- 이런 수정 과정은 딥러닝의 핵심 알고리즘인 **역전파(Backpropagation) 알고리즘**을 구현한 **옵티마이저(optimizer)** 가 담당함

## 특징
1. 데이터의 좋은 표현을 만드는 특성 공학을 자동화함
- 특성을 직접 찾는 대신 한 번에 모든 특성을 학습할 수 있음
- 머신러닝 작업 흐름을 매우 단순화함
- 고도의 다단계 작업 과정을 하나의 간단한 엔드-투-엔드(end-to-end) 딥러닝 모델로 대체할 수 있음

2. 층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다는 것과 이런 점진적인 중간 표현이 공동으로 학습됨

3. 각 층은 상위 층과 하위 층의 표현이 변함에 따라서 함께 바뀜

## 학습 과정
1단계. 데이터 준비<br>

2단계. 모델 컴파일
- 활성화함수, 손실함수, 옵티마이저 선택<br>

3단계. 모델 훈련<br>

4단계. 모델 예측

<br>

# 퍼셉트론(Perceptron)
- 신경망을 이루는 가장 중요한 기본 단위
- 입력값과 활성화 함수를 사용해 출력값을 다음으로 넘기는 가장 작은 신경망 단위
- 가중치(weight)는 입력신호가 결과 출력에 주는 영향도를 조절하는 매개변수
    - 가중치가 각각의 입력신호에 부여되는 입력신호와 계산을 하고 신호의 총합이 정해진 임계값을 넘으면 다음 뉴런으로 신호를 전달하고 아니면 아무것도 수행하지 않음
    - 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 **활성함수**에 의해 판단됨

- 가중합
$$w_1x_1+w_2x_2+b$$
    - 기울기: 가중치를 의미하는 $w$(weight)
    - 절편: $b$(bias)

#### 활성화 함수(activation function)
- 가중합의 결과를 놓고 1 또는 0을 출력해서 다음으로 보냄
- 여기서 0과 1을 판단하는 함수

## 퍼셉트론의 변화 => 층 추가
- **은닉층(hidden layer)** 을 만들면 됨
- 입력층->은닉층->출력층으로 결과값 전달

<br>

# 오차 역전파(Backpropagation)
## 가중치 조정
#### (1) 단일 퍼셉트론 가중치 조정
- 결과값을 얻으면 실제값과의 오차를 구해 앞 단계에서 정한 가중치를 조정

#### (2) 다층 퍼셉트론의 가중치 조정
- 가중치 업데이트: 출력층과 은닉층의 가중치 수정
- 결과값의 오차를 구해 이를 토대로 하나 앞선 가중치를 차례로 거슬러 올라가며 조정함

### 오차 역전파 동작 단계
1. 임의의 초기 가중치($W$)를 준 뒤 결과($y_{out}$)를 계산
2. 계산 결과(예측값)와 원하는 값(실제값, 타깃값) 사이의 오차 계산
3. 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트
4. 위 과정을 오차가 줄어들지 않을 때까지 반복

**오차가 작아지는 방향**<br>
⇔ 기울기가 0이 되는 방향

<br>

# 기울기 소실 문제와 활성화 함수
## 기울기 소실 문제
- 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing gradient) 문제가 발생

## 활성화 함수
### 시그모이드(sigmoid) 함수
- 여러 층을 거칠수록 기울기가 사라져 가중치를 수정하기가 어려워짐

<br>

⇒ 기울기 소실 문제를 해결하기 위해 시그모이들 대체하는 활성화 함수들
### 1. 하이퍼볼릭 탄젠트(tanh) 함수
- 시그모이드 함수의 범위를 -1에서 1로 확장
- 미분시 1보다 작은 값이 존재하여 기울기 소실 문제는 사라지지 않음

### 2. 렐루(ReLU) 함수
- 0미만의 값은 모두 0으로 처리하고, 0 이상의 값은 원본 값 그대로 출력
- 현재 가장 많이 사용되는 활성화 함수
- 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고 남아있을 수 있음

### 3. 소프트플러스(softplus) 함수
- 렐루의 0이 되는 순간을 완화

<br>

# 경사하강법과 옵티마이저
- 속도와 정확도 문제를 해결
### 확률적 경사하강법(SGD)
- 속도 개선
```python
keras.optimizer.SGD(lr=0.1)
```

### 모멘텀(momentum)
- 정확도 개선
- 관성의 방향을 고려해 진동과 폭을 줄이는 효과
- 오차를 수정하기 전 바로 앞 수정 값과 방향을 참고하여 같은 방향으로 일정한 비율만 수정
- 이전 이동 값을 고려하여 일정 비율만큼만 다음 값을 결정하므로 관성의 효과를 낼 수 있음
```python
keras.optimizer.SGD(lr=0.1, momentum=0.9)
```

### 네스테로프 모멘텀(NAG, Nesterov Accelrated Gradient)
- 정확도 개선
- 모멘텀이 이동시킬 방향으로 미리 이동해서 그레디언트 계산
- 불필요한 이동을 줄이는 효과
```python
keras.optimizer.SGD(lr=0.1, momentum=0.9, nestrov=True)
```

### 아다그라드(Adagrad, Adaptive Gradient)
- 보폭 크기 개선
- 변수의 업데이트가 잦을 경우 학습률을 적게하여 이동 보폭을 조절하는 방법
```python
keras.optimizer.Adagrad(lr=0.01, epsilon=1e-6)
```
### RMSprop
- 보폭 크기 개선
- 아다그라드의 보폭 민감도를 보완하는 방법
```python
keras.optimizer.RMSprop(lr=0.001, rho=0.9, epsilon=1e-8, decay=0.0)
```

### 아담(Adam, Adaptive Moment Estimation)
- 정확도와 보폭 크기 개선
- 모멘텀과 RMSprop 방법을 결합한 방법
- 자주 사용
```python
keras.optimizer.Adam(lr=0.001, beta_1=0.9, beta_2=0.9999, epsilon=1e-08, decay=0.0)
```


## 작업흐름
### 1. 문제정의와 데이터 수집
- 문제정의: 분류(이진, 다중), 회귀, 군집, 생성, 강화, 차원축소 …
- 입력(X), 출력(y)

### 2. 성능지표 선택
- 분류문제
    - 클래스 분포가 균일한 경우: 정확도, ROC-AUC
    - 클래스 분포가 균일하지 않은 경우: 정밀도, 재현율
    - 랭킹(순위), 다중분류의 문제일 경우: 평균정밀도
- 회귀문제
    - MSE, MAE, MAPE, R2 등
- 군집문제
    - 측정지표가 없음
    - 군집결과가 비즈니스에 기여하는 정도를 측정

### 3. 평가 방법 선택
- 홀드아웃 검증세트 분리: 데이터가 풍부할 때
- K-fold 교차 검증: 홀드아웃 검증을 수행하기엔 샘플의 수가 너무 적을 때
- 반복 K-fold 교차 검증: 데이터가 적고 매우 정확한 모델 평가가 필요할 때

### 4. 데이터 준비
- 텐서로 구성(벡터화)
- 작은 값으로 스케일을 조정: 정규화(MinMax)
- 특성마다 범위가 다른 경우: 표준화(Standard)

### 5. 기본보다 나은 모델 훈련
- 통계 검정력
    - 아주 단순한 모델보다 나은 수준의 모델을 개발
    - 검정력: 가설이 참일 때 채택할 확률
    - 데이터 셋에 있는 클래스별 분포보다 모델의 정확도가 높아야 세운 가설이 옳다고 할 수 있음
- 두 개의 가설
    - 주어진 입력으로 출력을 예측할 수 있다.
    - 가용한 데이터에 입력과 출력 사이의 관계를 학습하는데 충분한 정보가 있다.
- 마지막 층의 활성화 함수: 네트워크 출력에 필요한 제한을 가함
    - 이진분류: 시그모이드 함수
    - 단일레이블 다중분류: 소프트맥스 함수
    - 다중레이블 다중분류: 시그모이드 함수
    - 회귀: 사용하지 않음
        - 0~1 사이의 값에 대한 회귀: 시그모이드 함수
- 손실함수: 풀려고 하는 문제의 종류에 따라서 적합한 것 지정
    - 이진분류: binary_crossentropy
    - 단일레이블 다중분류: categorical_crossentropy, sparse_categorical_crossentropy
    - 다중레이블 다중분류: binary_crossentropy
    - 임의의 값에 대한 회귀: mse
    - 0~1사이의 값에 대한 회귀: mse, binary_crossentropy
- 최적화
    - optimizer: SGD, momentum, nestrov, PMSprop, Adagrad, Adam
    - learning rate

### 6. 과대적합 모델 구축
- 주어진 문제를 적절히 모델링하기에 충분한 층과 파라미터가 있는가?
- 최적화와 일반화 사이의 줄다리기
    - 과소용량과 과대용량 경계의 적절한 위치의 모델이 이상적
- 큰 모델 만들기
    - 층을 추가
    - 층의 크기를 키움
    - 더 많은 epochs 동안 훈련
- 큰 모델로 훈련손실과 검증손실 모니터링
    - 검증데이터에서 모델 성능이 감소하기 시작했을 때 과대적합에 도달한 것임

### 7. 모델 규제와 하이퍼파라미터 튜닝
- 반복적으로 모델을 수정하고 훈련하고 검증 데이터에서 평가함
    - 드롭아웃 추가
    - 층을 추가하거나 제거해서 다른 구조 시도
    - L1, L2 규제 추가
    - 최적의 설정을 찾기 위해 하이퍼파라미터를 바꾸어 시도
        - 층의 유닛 수(뉴런/노드), 옵티마이저의 학습률 등
    -새로운 특성을 추가하거나 유용하지 않은 특성 제거
- 유의사항
    - 검증과정에서 얻은 피드백을 사용하여 모델을 튜닝할 때 검증과정에 대한 정보를 모델에 누설하고 있으므로, 몇 번 반복하는 것은 큰 문제가 되지 않으나 많이 반복하게 되면 모델이 검증 과정에 대해 과대적합되어 검증과정의 신뢰도가 떨어짐

### 8. 최종 모델 훈련
- 만족할만한 모델 설정이 확정되면 가용한 모든 데이터(훈련/검증)를 사용해서 최종 모델을 훈련

### 9. 테스트 데이터에서 평가
- 딱 한 번 테스트 세트에서 평가
- 테스트 세트의 성능이 검증 세트의 성능보다 많이 나쁘면 검증과정이 신뢰성이 없거나 모델의 하이퍼파라미터를 튜닝하는 동안 검증데이터에 과대적합이 발생된 경우임
